{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# R ML Crash Course_Part 3:  Data Transform, Split\n\n## Full Day Workshop for user learn Data Science with R\n### 2018  Timothy CL Lam\nThis is meant for internal usage, part of contents copied externally, not for commercial purpose\n", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Data Pre-Processing in R\n- The caret package in R provides a number of useful data transforms. These transforms can be\nused in two ways:\n\n**Standalone** : \n- Transforms can be modeled from training data and applied to multiple datasets.\nThe model of the transform is prepared using the preProcess() function and\n- applied to\na dataset using the predict() function.\n\n**Training** : \n- Transforms can be prepared and applied automatically during model evaluation.\nTransforms applied during training are prepared using the preProcess() function and\n- passed to the train() function via the preProcess argument.\n\n**Useful**\n- regression algorithms, \n- instance-based methods (like KNN and LVQ), support vector\nmachines and neural networks. \n- They are less likely to be useful for tree and rule-based methods.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Summary of Transform Methods\nBelow is a quick summary of all of the transform methods supported in the argument to the\npreProcess() function in caret.\n- BoxCox: apply a Box-Cox transform, values must be non-zero and positive.\n- YeoJohnson: apply a Yeo-Johnson transform, like a BoxCox, but values can be negative.\n- expoTrans: apply a power transform like BoxCox and YeoJohnson.\n- zv: remove attributes with a zero variance (all the same value).\n- nzv: remove attributes with a near zero variance (close to the same value).\n- center: subtract mean from values.\n- scale: divide values by standard deviation.\n- range: normalize values.\n- pca: transform data to the principal components.\n- ica: transform data to the independent components.\n- spatialSign: project data onto a unit circle.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Scale Data\n- The scale transform calculates the standard deviation for an attribute and divides each value by\nthat standard deviation. \n- This is a useful operation for scaling data with a Gaussian distribution\nconsistently.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 4, 
            "metadata": {}, 
            "outputs": [
                {
                    "traceback": [
                        "Error: package or namespace load failed for \u2018caret\u2019\nTraceback:\n", 
                        "1. library(caret)", 
                        "2. stop(gettextf(\"package or namespace load failed for %s\", sQuote(package)), \n .     call. = FALSE, domain = NA)"
                    ], 
                    "evalue": "Error: package or namespace load failed for \u2018caret\u2019\n", 
                    "ename": "ERROR", 
                    "output_type": "error"
                }
            ], 
            "source": "# load packages\nlibrary(caret)\n# load the dataset\ndata(iris)\n# summarize data\nsummary(iris[,1:4])\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(iris[,1:4], method=c(\"scale\"))\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, iris[,1:4])\n# summarize the transformed dataset\nsummary(transformed)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 5, 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "R version 3.3.2 (2016-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: CentOS Linux 7 (Core)\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_2.2.1   lattice_0.20-33 SparkR_2.1.2   \n\nloaded via a namespace (and not attached):\n [1] Rcpp_0.12.5        plyr_1.8.3         iterators_1.0.8    tools_3.3.2       \n [5] digest_0.6.13      uuid_0.1-2         jsonlite_1.5       evaluate_0.10.1   \n [9] tibble_1.1         gtable_0.2.0       nlme_3.1-126       Matrix_1.2-4      \n[13] foreach_1.4.3      IRdisplay_0.4.4    IRkernel_0.8.11    repr_0.9          \n[17] stringr_1.0.0      grid_3.3.2         nnet_7.3-12        R6_2.1.2          \n[21] minqa_1.2.4        pbdZMQ_0.2-3       reshape2_1.4.1     magrittr_1.5      \n[25] scales_0.4.1       codetools_0.2-14   ModelMetrics_1.1.0 MASS_7.3-48       \n[29] splines_3.3.2      assertthat_0.1     colorspace_1.2-6   stringi_1.0-1     \n[33] lazyeval_0.2.0     munsell_0.4.3      crayon_1.3.2      "
                    }
                }
            ], 
            "source": "sessionInfo()"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Center Data\n#### The center transform calculates the mean for an attribute and subtracts it from each value.m", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load packages\nlibrary(caret)\n# load the dataset\ndata(iris)\n# summarize data\nsummary(iris[,1:4])\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(iris[,1:4], method=c(\"center\"))\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, iris[,1:4])\n# summarize the transformed dataset\nsummary(transformed)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Standardize Data\n#### Combining the scale and center transforms will standardize your data. Attributes will have a mean value of 0 and a standard deviation of 1.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load packages\nlibrary(caret)\n# load the dataset\ndata(iris)\n# summarize data\nsummary(iris[,1:4])\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(iris[,1:4], method=c(\"center\", \"scale\"))\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, iris[,1:4])\n# summarize the transformed dataset\nsummary(transformed)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Normalize Data\n#### Data values can be scaled into the range of [0, 1] which is called normalization.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 8, 
            "metadata": {}, 
            "outputs": [
                {
                    "traceback": [
                        "Error: package or namespace load failed for \u2018caret\u2019\nTraceback:\n", 
                        "1. library(caret)", 
                        "2. stop(gettextf(\"package or namespace load failed for %s\", sQuote(package)), \n .     call. = FALSE, domain = NA)"
                    ], 
                    "evalue": "Error: package or namespace load failed for \u2018caret\u2019\n", 
                    "ename": "ERROR", 
                    "output_type": "error"
                }
            ], 
            "source": "# load packages\nlibrary(caret)\n# load the dataset\ndata(iris)\n# summarize data\nsummary(iris[,1:4])\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(iris[,1:4], method=c(\"range\"))\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, iris[,1:4])\n# summarize the transformed dataset\nsummary(transformed)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Box-Cox Transform\n- When an attribute has a Gaussian-like distribution but is shifted, this is called a skew. \n- The\ndistribution of an attribute can be shifted to reduce the skew and make it more Gaussian. \n- The\nBoxCox transform can perform this operation (assumes all values are positive).", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load packages\nlibrary(mlbench)\nlibrary(caret)\n# load the dataset\ndata(PimaIndiansDiabetes)\n# summarize pedigree and age\nsummary(PimaIndiansDiabetes[,7:8])\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c(\"BoxCox\"))\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])\n# summarize the transformed dataset (note pedigree and age)\nsummary(transformed)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Yeo-Johnson Transform\n#### The YeoJohnson transform another power-transform like Box-Cox, but it supports raw values that are equal to zero and negative.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load packages\nlibrary(mlbench)\nlibrary(caret)\n# load the dataset\ndata(PimaIndiansDiabetes)\n# summarize pedigree and age\nsummary(PimaIndiansDiabetes[,7:8])\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c(\"YeoJohnson\"))\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])\n# summarize the transformed dataset (note pedigree and age)\nsummary(transformed)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Principal Component Analysis Transform\n- The PCA transforms the data to return only the principal components, a technique from\nmultivariate statistics and linear algebra. \n- The transform keeps those components above the\nvariance threshold (default=0.95) or the number of components can be speci\fed (pcaComp).\n- The result is attributes that are uncorrelated, useful for algorithms like linear and generalized\nlinear regression.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load the packages\nlibrary(mlbench)\n# load the dataset\ndata(iris)\n# summarize dataset\nsummary(iris)\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(iris, method=c(\"center\", \"scale\", \"pca\"))\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, iris)\n# summarize the transformed dataset\nsummary(transformed)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Independent Component Analysis Transform\n- Transform the data to the independent components. \n- Unlike PCA, ICA retains those components\nthat are independent. \n- You must specify the number of desired independent components with\nthe n.comp argument. \n- This transform may be useful for algorithms such as Naive Bayes.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load packages\nlibrary(mlbench)\nlibrary(caret)\n# load the dataset\ndata(PimaIndiansDiabetes)\n# summarize dataset\nsummary(PimaIndiansDiabetes[,1:8])\n# calculate the pre-process parameters from the dataset\npreprocessParams <- preProcess(PimaIndiansDiabetes[,1:8], method=c(\"center\", \"scale\",\n    \"ica\"), n.comp=5)\n\n# summarize transform parameters\nprint(preprocessParams)\n# transform the dataset using the parameters\ntransformed <- predict(preprocessParams, PimaIndiansDiabetes[,1:8])\n# summarize the transformed dataset\nsummary(transformed)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Resampling Methods To Estimate Model Accuracy\nThe example below splits the iris dataset so that 80% is used for training a Naive Bayes\nmodel and 20% is used to evaluate the model's performance.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 10, 
            "metadata": {}, 
            "outputs": [
                {
                    "traceback": [
                        "Error: package or namespace load failed for \u2018caret\u2019\nTraceback:\n", 
                        "1. library(caret)", 
                        "2. stop(gettextf(\"package or namespace load failed for %s\", sQuote(package)), \n .     call. = FALSE, domain = NA)"
                    ], 
                    "evalue": "Error: package or namespace load failed for \u2018caret\u2019\n", 
                    "ename": "ERROR", 
                    "output_type": "error"
                }
            ], 
            "source": "# load the packages\nlibrary(caret)\nlibrary(klaR)\n# load the iris dataset\ndata(iris)\n# define an 80%/20% train/test split of the dataset\ntrainIndex <- createDataPartition(iris$Species, p=0.80, list=FALSE)\ndataTrain <- iris[ trainIndex,]\ndataTest <- iris[-trainIndex,]\n# train a naive Bayes model\nfit <- NaiveBayes(Species~., data=dataTrain)\n# make predictions\npredictions <- predict(fit, dataTest[,1:4])\n# summarize results\nconfusionMatrix(predictions$class, dataTest$Species)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Bootstrap\n- Bootstrap resampling involves taking random samples from the dataset (with re-selection)\nagainst which to evaluate the model. \n- In aggregate, the results provide an indication of the\nvariance of the model's performance. \n- Typically, large number of resampling iterations are\nperformed (thousands or tens of thousands). \n- The following example uses a bootstrap with 100\nresamples to estimate the accuracy of a Naive Bayes model.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 11, 
            "metadata": {}, 
            "outputs": [
                {
                    "traceback": [
                        "Error: package or namespace load failed for \u2018caret\u2019\nTraceback:\n", 
                        "1. library(caret)", 
                        "2. stop(gettextf(\"package or namespace load failed for %s\", sQuote(package)), \n .     call. = FALSE, domain = NA)"
                    ], 
                    "evalue": "Error: package or namespace load failed for \u2018caret\u2019\n", 
                    "ename": "ERROR", 
                    "output_type": "error"
                }
            ], 
            "source": "# load the package\nlibrary(caret)\n# load the iris dataset\ndata(iris)\n# define training control\ntrainControl <- trainControl(method=\"boot\", number=100)\n# evalaute the model\nfit <- train(Species~., data=iris, trControl=trainControl, method=\"nb\")\n# display the results\nprint(fit)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "# k-fold Cross-Validation\n- The k-fold cross-validation method involves splitting the dataset into k-subsets. Each subset\nis held-out while the model is trained on all other subsets. \n- This process is repeated until\naccuracy is determined for each instance in the dataset, and an overall accuracy estimate is\nprovided. \n- It is a robust method for estimating accuracy, and the size of k can tune the amount\nof bias in the estimate, with popular values set to 5 and 10. \n- The following example uses 10-fold\ncross-validation to estimate the accuracy of the Naive Bayes algorithm on the iris dataset.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load the package\nlibrary(caret)\n# load the iris dataset\ndata(iris)\n# define training control\ntrainControl <- trainControl(method=\"cv\", number=10)\n# evaluate the model\nfit <- train(Species~., data=iris, trControl=trainControl, method=\"nb\")\n# display the results\nprint(fit)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Repeated k-fold Cross-Validation\n- The process of splitting the data into k-folds can be repeated a number of times, this is called\nRepeated k-fold Cross-Validation. \n- The final model accuracy is taken as the mean from the\nnumber of repeats. \n- The following example demonstrates 10-fold cross-validation with 3 repeats\nto estimate the accuracy of the Naive Bayes algorithm on the iris datase", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load the package\nlibrary(caret)\n# load the iris dataset\ndata(iris)\n# define training control\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n# evaluate the model\nfit <- train(Species~., data=iris, trControl=trainControl, method=\"nb\")\n# display the results\nprint(fit)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Summary\n## Tips For Evaluating Algorithms\n\n- Using a data split into a training and test set is a good idea when you have a lot of data\nand you are con\fdent that your training sample is representative of the larger dataset.\n- Using a data split is very e\u000ecient and is often used to get a quick estimate of model\naccuracy.\n- Cross-validation is a gold standard for evaluating model accuracy, often with k-folds set\nto 5 or 10 to balance over\ftting the training data with a fair accuracy estimate.\n- Repeated k-fold cross-validation is preferred when you can a\u000bord the computational\nexpense and require a less biased estimate.", 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "R", 
            "name": "r-spark21", 
            "display_name": "R with Spark 2.1"
        }, 
        "language_info": {
            "pygments_lexer": "r", 
            "mimetype": "text/x-r-source", 
            "name": "R", 
            "version": "3.3.2", 
            "file_extension": ".r", 
            "codemirror_mode": "r"
        }
    }, 
    "nbformat_minor": 1
}